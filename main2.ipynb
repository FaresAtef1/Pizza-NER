{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook we will do some preprocessing on the data and tokenization\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"\n",
    "    Cleans the input string by removing special characters, and unnecessary punctuation.\n",
    "\n",
    "    Args:\n",
    "        input_string: The string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned string.\n",
    "    \"\"\"\n",
    "    # Remove special characters and unnecessary punctuation\n",
    "    # TODO: Add more special characters as needed to be excluded\n",
    "    cleaned_string = re.sub(r\"[^\\w\\s'-]\", \"\", input_string)  # Keeps only alphanumeric characters and spaces and apostrophes and hyphens\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    # Remove extra whitespace\n",
    "    cleaned_string = re.sub(r\"\\s+\", \" \", cleaned_string).strip()\n",
    "    return cleaned_string\n",
    "\n",
    "def tokenize_string(input_string):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string into tokens.\n",
    "    \n",
    "    Args:\n",
    "        input_string: The string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(input_string)\n",
    "    return tokens\n",
    "\n",
    "def label_tokens1(input_tokens, structure_text):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attribute_values = {\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\", \"QUANTITY\"}\n",
    "    structure_map = {}\n",
    "    for attribute in attribute_values:\n",
    "        # Match the attribute and its value in the structure text\n",
    "        pattern = r\"\\(\\s*\"+ attribute + r\"\\s+([^\\)]*)\\s*\\)\"\n",
    "        matches = re.finditer(pattern, structure_text)\n",
    "        for match in matches:\n",
    "            value = match.group(1).strip()\n",
    "            # Special handling for TOPPING with \"not\" before it\n",
    "            if attribute == \"TOPPING\":\n",
    "                preceding_text = structure_text[:match.start()]\n",
    "                if re.search(r\"\\bNOT\\b\\s*\\($\", preceding_text, re.IGNORECASE):\n",
    "                    attribute = \"NOT_TOPPING\"\n",
    "            structure_map[value] = attribute\n",
    "    labeled_output = []\n",
    "    labeled_output_nums = []\n",
    "    entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17, \"NONE\": 18}\n",
    "    for token in input_tokens:\n",
    "        label = \"NONE\"\n",
    "        if token in structure_map:\n",
    "            label = structure_map[token]\n",
    "            label = \"B_\" + label\n",
    "        # else check if it is part of the key\n",
    "        else:\n",
    "            for key in structure_map.keys():\n",
    "                if token in key.split():\n",
    "                    label = structure_map[key]\n",
    "                    if token == key.split()[0]:\n",
    "                        label = \"B_\" + label\n",
    "                    else:\n",
    "                        label = \"I_\" + label\n",
    "                    break\n",
    "        labeled_output.append((token, label))\n",
    "        labeled_output_nums.append(entity_to_num[label])\n",
    "    return labeled_output, labeled_output_nums\n",
    "\n",
    "def label_tokens2(input_tokens, structure_tokens):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attributes = [\"PIZZAORDER\", \"DRINKORDER\", \"COMPLEX_TOPPING\"]\n",
    "    execluded = {\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\", \"QUANTITY\"}\n",
    "    curr = \"NONE\"\n",
    "    # I will also keep tracking \"(\" and \")\" to know when to change the current attribute to NONE\n",
    "    parentheses =0\n",
    "    is_begin = True\n",
    "    labels_mapping = {}\n",
    "    for token in structure_tokens:\n",
    "        if token in attributes:\n",
    "            curr = token\n",
    "            is_begin = True\n",
    "        elif token == \"(\":\n",
    "            parentheses += 1\n",
    "        elif token == \")\":\n",
    "            parentheses -= 1\n",
    "            if parentheses == 1:\n",
    "                curr = \"NONE\"\n",
    "        elif token not in execluded:\n",
    "            if curr == \"NONE\":\n",
    "                labels_mapping[token] = curr\n",
    "            elif is_begin:\n",
    "                labels_mapping[token] = \"B_\" + curr\n",
    "                is_begin = False\n",
    "            else:\n",
    "                labels_mapping[token] = \"I_\" + curr\n",
    "    labeled_output = []\n",
    "    labeled_output_nums =[]\n",
    "    intent_to_num = {\"I_PIZZAORDER\": 0, \"I_DRINKORDER\": 1, \"I_COMPLEX_TOPPING\": 2, \"B_PIZZAORDER\": 3, \"B_DRINKORDER\": 4, \"B_COMPLEX_TOPPING\": 5, \"NONE\": 6}\n",
    "    for token in input_tokens:\n",
    "        label = \"NONE\"\n",
    "        if token in labels_mapping:\n",
    "            label = labels_mapping[token]\n",
    "        labeled_output.append((token, label))\n",
    "        labeled_output_nums.append(intent_to_num[label])\n",
    "    return labeled_output, labeled_output_nums\n",
    "\n",
    "def label_input(input_text, structure_text1, structure_text2):\n",
    "    \"\"\"\n",
    "    It is a similar function to the previous one, but it is used for adding another layer for the input\n",
    "    which is the preprocessing of the input text and then tokenizing it.\n",
    "\n",
    "    Args:\n",
    "        input_text: The raw input text.\n",
    "        structure_text1: The structured text containing attributes and their values. (train.TOP-DECOUPLED)\n",
    "        structure_text2: The structured text containing attributes and their values. (train.TOP)\n",
    "    \n",
    "    Returns:\n",
    "        2 lists of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_string(input_text)\n",
    "    input_tokens = tokenize_string(cleaned_text)\n",
    "    labeled_output1 = label_tokens1(input_tokens, structure_text1)\n",
    "    structure2_tokens = tokenize_string(structure_text2)\n",
    "    labeled_output2 = label_tokens2(input_tokens, structure2_tokens)\n",
    "    return labeled_output1, labeled_output2\n",
    "\n",
    "def label_complete_input (input_list, structure_text1_list, structure_text2_list):\n",
    "    \"\"\"\n",
    "    It is a similar function to the previous one, but it takes inputs as lists of tokens instead of strings.\n",
    "\n",
    "    Args:\n",
    "        input_text: The raw input text.\n",
    "        structure_text1: The structured text containing attributes and their values. (train.TOP-DECOUPLED)\n",
    "        structure_text2: The structured text containing attributes and their values. (train.TOP)\n",
    "    \n",
    "    Returns:\n",
    "        3 lists of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    labeled_output1 = []\n",
    "    labeled_output2 = []\n",
    "    for text, struct1, struct2 in zip(input_list, structure_text1_list, structure_text2_list):\n",
    "        cleaned_text = clean_string(text)\n",
    "        input_tokens = tokenize_string(cleaned_text)\n",
    "        _, labels = label_tokens1(input_tokens, struct1)\n",
    "        labeled_output1.append(labels)\n",
    "        structure2_tokens = tokenize_string(struct2)\n",
    "        _, labels = label_tokens2(input_tokens, structure2_tokens)\n",
    "        labeled_output2.append(labels)\n",
    "    return labeled_output1, labeled_output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------1------------------\n",
      "1  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('large', 'B_SIZE'), ('vegetarian', 'B_STYLE'), ('pizza', 'NONE')], [18, 18, 18, 8, 9, 11, 18])\n",
      "2  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_PIZZAORDER'), ('large', 'I_PIZZAORDER'), ('vegetarian', 'I_PIZZAORDER'), ('pizza', 'I_PIZZAORDER')], [6, 6, 6, 3, 0, 0, 0])\n",
      "------------------2------------------\n",
      "1  ([('a', 'B_NUMBER'), ('20', 'B_VOLUME'), ('fl', 'I_VOLUME'), ('ounce', 'I_VOLUME'), ('cherry', 'B_DRINKTYPE'), ('coke', 'I_DRINKTYPE'), ('bottle', 'B_CONTAINERTYPE')], [8, 14, 6, 6, 12, 4, 13])\n",
      "2  ([('a', 'B_DRINKORDER'), ('20', 'I_DRINKORDER'), ('fl', 'I_DRINKORDER'), ('ounce', 'I_DRINKORDER'), ('cherry', 'I_DRINKORDER'), ('coke', 'I_DRINKORDER'), ('bottle', 'I_DRINKORDER')], [4, 1, 1, 1, 1, 1, 1])\n",
      "------------------3------------------\n",
      "1  ([('four', 'B_NUMBER'), ('pizzas', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('also', 'NONE'), ('three', 'B_NUMBER'), ('cans', 'B_CONTAINERTYPE'), ('of', 'NONE'), ('ice', 'B_DRINKTYPE'), ('tea', 'I_DRINKTYPE'), ('and', 'NONE'), ('three', 'B_NUMBER'), ('regular', 'B_SIZE'), ('san', 'B_DRINKTYPE'), ('pellegrinos', 'I_DRINKTYPE')], [8, 18, 18, 10, 2, 18, 18, 8, 13, 18, 12, 4, 18, 8, 9, 12, 4])\n",
      "2  ([('four', 'B_PIZZAORDER'), ('pizzas', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'NONE'), ('also', 'NONE'), ('three', 'B_DRINKORDER'), ('cans', 'I_DRINKORDER'), ('of', 'I_DRINKORDER'), ('ice', 'I_DRINKORDER'), ('tea', 'I_DRINKORDER'), ('and', 'NONE'), ('three', 'B_DRINKORDER'), ('regular', 'I_DRINKORDER'), ('san', 'I_DRINKORDER'), ('pellegrinos', 'I_DRINKORDER')], [3, 0, 0, 0, 0, 6, 6, 4, 1, 1, 1, 1, 6, 4, 1, 1, 1])\n",
      "------------------4------------------\n",
      "1  ([('i', 'NONE'), ('want', 'NONE'), ('one', 'B_NUMBER'), ('personal', 'B_SIZE'), ('-', 'I_SIZE'), ('size', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('any', 'NONE'), ('carrots', 'B_TOPPING')], [18, 18, 8, 9, 1, 1, 18, 18, 18, 10])\n",
      "2  ([('i', 'NONE'), ('want', 'NONE'), ('one', 'B_PIZZAORDER'), ('personal', 'I_PIZZAORDER'), ('-', 'I_PIZZAORDER'), ('size', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('any', 'I_PIZZAORDER'), ('carrots', 'I_PIZZAORDER')], [6, 6, 3, 0, 0, 0, 0, 0, 0, 0])\n",
      "------------------5------------------\n",
      "1  ([('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_NUMBER'), ('high', 'B_STYLE'), ('rise', 'I_STYLE'), ('dough', 'I_STYLE'), ('pie', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('a', 'B_QUANTITY'), ('lot', 'I_QUANTITY'), ('of', 'I_QUANTITY'), ('meatball', 'B_TOPPING')], [18, 18, 18, 8, 11, 3, 3, 18, 18, 10, 2, 18, 15, 7, 7, 10])\n",
      "2  ([('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_PIZZAORDER'), ('high', 'I_PIZZAORDER'), ('rise', 'I_PIZZAORDER'), ('dough', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'I_PIZZAORDER'), ('a', 'B_COMPLEX_TOPPING'), ('lot', 'I_COMPLEX_TOPPING'), ('of', 'I_COMPLEX_TOPPING'), ('meatball', 'I_COMPLEX_TOPPING')], [6, 6, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 2, 2])\n",
      "------------------6------------------\n",
      "1  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('lunch', 'B_SIZE'), ('-', 'I_SIZE'), ('sized', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('alfredo', 'B_TOPPING'), ('chicken', 'I_TOPPING')], [18, 18, 18, 8, 9, 1, 1, 18, 18, 10, 2])\n",
      "2  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_PIZZAORDER'), ('lunch', 'I_PIZZAORDER'), ('-', 'I_PIZZAORDER'), ('sized', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('alfredo', 'I_PIZZAORDER'), ('chicken', 'I_PIZZAORDER')], [6, 6, 6, 3, 0, 0, 0, 0, 0, 0, 0])\n",
      "------------------7------------------\n",
      "1  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('lunch', 'B_SIZE'), ('-', 'I_SIZE'), ('sized', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('alfredo', 'B_TOPPING'), ('chicken', 'I_TOPPING'), ('or', 'NONE'), ('beef', 'B_TOPPING')], [18, 18, 18, 8, 9, 1, 1, 18, 18, 10, 2, 18, 10])\n",
      "2  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_PIZZAORDER'), ('lunch', 'I_PIZZAORDER'), ('-', 'I_PIZZAORDER'), ('sized', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('alfredo', 'I_PIZZAORDER'), ('chicken', 'I_PIZZAORDER'), ('or', 'I_PIZZAORDER'), ('beef', 'I_PIZZAORDER')], [6, 6, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "------------------8------------------\n",
      "1  ([('pie', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('with', 'NONE'), ('not', 'B_QUANTITY'), ('much', 'I_QUANTITY'), ('parmesan', 'B_TOPPING'), ('cheese', 'I_TOPPING')], [18, 18, 10, 2, 18, 18, 15, 7, 10, 2])\n",
      "2  ([('pie', 'B_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_COMPLEX_TOPPING'), ('and', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('not', 'B_COMPLEX_TOPPING'), ('much', 'I_COMPLEX_TOPPING'), ('parmesan', 'I_COMPLEX_TOPPING'), ('cheese', 'I_COMPLEX_TOPPING')], [3, 0, 0, 2, 0, 0, 5, 2, 2, 2])\n",
      "------------------9------------------\n",
      "1  ([('pie', 'NONE'), ('without', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('with', 'NONE'), ('parmesan', 'B_TOPPING'), ('cheese', 'I_TOPPING')], [18, 18, 10, 2, 18, 18, 10, 2])\n",
      "2  ([('pie', 'B_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('parmesan', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER')], [3, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "out1,out2 = label_input(\"i'd like a large vegetarian pizza\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (STYLE vegetarian ) ) )\",\"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE large ) (STYLE vegetarian ) pizza ) )\")\n",
    "print(\"------------------1------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"a 20 fl ounce cherry coke bottle\", \"(ORDER (DRINKORDER (NUMBER a ) (VOLUME 20 fl ounce ) (DRINKTYPE cherry coke ) (CONTAINERTYPE bottle ) ) )\", \"(ORDER (DRINKORDER (NUMBER a ) (VOLUME 20 fl ounce ) (DRINKTYPE cherry coke ) (CONTAINERTYPE bottle ) ) )\")\n",
    "print(\"------------------2------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"four pizzas with american cheese and also three cans of ice tea and three regular san pellegrinos\", \"(ORDER (PIZZAORDER (NUMBER four ) (TOPPING american cheese ) ) (DRINKORDER (NUMBER three ) (CONTAINERTYPE cans ) (DRINKTYPE ice tea ) ) (DRINKORDER (NUMBER three ) (SIZE regular ) (DRINKTYPE san pellegrinos ) ) )\", \"(ORDER (PIZZAORDER (NUMBER four ) pizzas with (TOPPING american cheese ) ) and also (DRINKORDER (NUMBER three ) (CONTAINERTYPE cans ) of (DRINKTYPE ice tea ) ) and (DRINKORDER (NUMBER three ) (SIZE regular ) (DRINKTYPE san pellegrinos ) ) )\")\n",
    "print(\"------------------3------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"i want one personal - size pie without any carrots\", \"(ORDER (PIZZAORDER (NUMBER one ) (SIZE personal - size ) (NOT (TOPPING carrots ) ) ) )\", \"(ORDER i want (PIZZAORDER (NUMBER one ) (SIZE personal - size ) pie without any (NOT (TOPPING carrots ) ) ) )\")\n",
    "print(\"------------------4------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"can i have one high rise dough pie with american cheese and a lot of meatball\", \"(ORDER (PIZZAORDER (NUMBER one ) (STYLE high rise dough ) (TOPPING american cheese ) (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING meatball ) ) ) )\", \"(ORDER can i have (PIZZAORDER (NUMBER one ) (STYLE high rise dough ) pie with (TOPPING american cheese ) and (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING meatball ) ) ) )\") \n",
    "print(\"------------------5------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"i'd like a lunch - sized pie without alfredo chicken\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) (NOT (TOPPING alfredo chicken ) ) ) )\", \"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) pie without (NOT (TOPPING alfredo chicken ) ) ) )\") \n",
    "print(\"------------------6------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"i'd like a lunch - sized pie without alfredo chicken or beef\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) (NOT (TOPPING alfredo chicken ) )(NOT (TOPPING beef ) ) ) )\", \"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) pie without (NOT (TOPPING alfredo chicken ) ) or (NOT (TOPPING beef ) ) ) )\") \n",
    "print(\"------------------7------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"pie with american cheese and with not much parmesan cheese\", \"(ORDER (PIZZAORDER (TOPPING american cheese ) (COMPLEX_TOPPING (QUANTITY not much ) (TOPPING parmesan cheese ) ) ) )\", \"(ORDER (PIZZAORDER pie with (TOPPING american cheese ) and with (COMPLEX_TOPPING (QUANTITY not much ) (TOPPING parmesan cheese ) ) ) )\") \n",
    "print(\"------------------8------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"pie without american cheese and with parmesan cheese\", \"(ORDER (PIZZAORDER (NOT(TOPPING american cheese )) (TOPPING parmesan cheese ) ) )\", \"(ORDER (PIZZAORDER pie without (NOT(TOPPING american cheese )) and with (TOPPING parmesan cheese ) ) )\") \n",
    "print(\"------------------9------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def fix_json_file(path):\n",
    "    \"\"\"\n",
    "    Fixes a corrupted JSON file by formatting it properly.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the corrupted JSON file.\n",
    "\n",
    "    Returns:\n",
    "        None. Writes a corrected version of the JSON file to disk.\n",
    "    \"\"\"\n",
    "    fixed_file = open(\"../fixed_train.json\", \"w\")\n",
    "    fixed_file.write(\"[\\n\")\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            fixed_file.write(line[:-1] + \",\\n\")\n",
    "    fixed_file.seek(fixed_file.tell() - 3)\n",
    "    fixed_file.truncate()\n",
    "    fixed_file.write(\"]\")\n",
    "    fixed_file.close()\n",
    "fix_json_file(\"../data/fixed_PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and loads its content into a Python object.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Parsed JSON data as a Python object.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        part_of_data = data[:100]\n",
    "    return part_of_data\n",
    "data = read_data(\"../data/fixed_PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_corpus_from_json(data):\n",
    "    \"\"\"\n",
    "    Builds a training corpus from a JSON-like dataset.\n",
    "    Extracts the \"train.SRC\" field from each item in the dataset.\n",
    "\n",
    "    Args:\n",
    "        data: List of dictionaries, where each dictionary contains a \"train.SRC\" key.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the training corpus.\n",
    "    \"\"\"\n",
    "    src, top, decoupled = [], [], []\n",
    "    for d in data:\n",
    "        src.append(d[\"train.SRC\"])\n",
    "        top.append(d[\"train.TOP\"])\n",
    "        decoupled.append(d[\"train.TOP-DECOUPLED\"])\n",
    "    return src, top, decoupled\n",
    "src, top, decoupled = build_train_corpus_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can i have a large bbq pulled pork\n",
      "(ORDER can i have (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\n",
      "(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\n"
     ]
    }
   ],
   "source": [
    "print(src[0])\n",
    "print(top[0])\n",
    "print(decoupled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 18, 18, 8, 9, 10, 2, 2]\n",
      "[6, 6, 6, 3, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "entites, intents = label_complete_input(src, top, decoupled)\n",
    "# assert that the length of the entities and intents is the same as the length of the src for each one\n",
    "print(entites[0])\n",
    "print(intents[0])\n",
    "for src_, ent_, intent_ in zip(src, entites, intents):\n",
    "    assert len(tokenize_string(src_)) == len(ent_) == len(intent_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText  # For Word2Vec model\n",
    "import gensim  # General Gensim utilities\n",
    "import nltk  # For tokenization and natural language processing\n",
    "import json  # For handling JSON files\n",
    "# from transformers import BertTokenizer, BertModel  # BERT tokenizer and model\n",
    "# import torch  # For PyTorch tensors and operations\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters for the Word2Vec model\n",
    "VECTOR_SIZE = 50  # Size of word vectors\n",
    "WINDOW_SIZE = 5  # Context window size\n",
    "THREADS = 4  # Number of threads to use for training\n",
    "CUTOFF_FREQ = 1  # Minimum frequency for a word to be included in vocabulary\n",
    "EPOCHS = 100  # Number of training epochs\n",
    "\n",
    "def list_of_lists(sentences):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into a list of tokenized sentences.\n",
    "    Each sentence is split into individual words.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of strings where each string is a sentence.\n",
    "\n",
    "    Returns:\n",
    "        List of lists where each inner list contains tokens of a sentence.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sentence))\n",
    "    return tokenized_sentences\n",
    "\n",
    "def train_gensim_w2v_model(corpus):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the given corpus of sentences.\n",
    "\n",
    "    Args:\n",
    "        corpus: List of sentences (strings).\n",
    "\n",
    "    Returns:\n",
    "        A trained Gensim Word2Vec model.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = list_of_lists(corpus)\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_sentences,\n",
    "        vector_size=VECTOR_SIZE,\n",
    "        window=WINDOW_SIZE,\n",
    "        min_count=CUTOFF_FREQ,\n",
    "        workers=THREADS,\n",
    "    )\n",
    "    model.build_vocab(tokenized_sentences)\n",
    "    model.train(\n",
    "        corpus_iterable=tokenized_sentences,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=EPOCHS,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def embed_gensim(model, word):\n",
    "    \"\"\"\n",
    "    Retrieves the word embedding for a given word using a trained Gensim model.\n",
    "    Works for both w2v and fastext.\n",
    "    Args:\n",
    "        model: Trained Gensim Word2Vec model.\n",
    "        word: Word to retrieve the embedding for.\n",
    "\n",
    "    Returns:\n",
    "        Word embedding as a vector.\n",
    "    \"\"\"\n",
    "    return model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load('./Models/model_100k.pth', weights_only=False, map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17, \"NONE\": 18}\n",
    "intent_to_num = {\"I_PIZZAORDER\": 0, \"I_DRINKORDER\": 1, \"I_COMPLEX_TOPPING\": 2, \"B_PIZZAORDER\": 3, \"B_DRINKORDER\": 4, \"B_COMPLEX_TOPPING\": 5, \"NONE\": 6}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
