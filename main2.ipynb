{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook we will do some preprocessing on the data and tokenization\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"\n",
    "    Cleans the input string by removing special characters, and unnecessary punctuation.\n",
    "\n",
    "    Args:\n",
    "        input_string: The string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned string.\n",
    "    \"\"\"\n",
    "    # Remove special characters and unnecessary punctuation\n",
    "    # TODO: Add more special characters as needed to be excluded\n",
    "    cleaned_string = re.sub(r\"[^\\w\\s'-]\", \"\", input_string)  # Keeps only alphanumeric characters and spaces and apostrophes and hyphens\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    # Remove extra whitespace\n",
    "    cleaned_string = re.sub(r\"\\s+\", \" \", cleaned_string).strip()\n",
    "    return cleaned_string\n",
    "\n",
    "def tokenize_string(input_string):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string into tokens.\n",
    "    \n",
    "    Args:\n",
    "        input_string: The string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(input_string)\n",
    "    return tokens\n",
    "\n",
    "def label_tokens1(input_tokens, structure_tokens):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values as tokens.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attribute_values = {\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\", \"QUANTITY\"}\n",
    "    execluded = [\"PIZZAORDER\", \"DRINKORDER\", \"COMPLEX_TOPPING\"]\n",
    "    token_label = []\n",
    "    curr_attr = \"NONE\"\n",
    "    is_not_topping = False\n",
    "    not_parentheses = 0\n",
    "    is_begin = True\n",
    "    for struct_token in structure_tokens:\n",
    "        if struct_token == \"NOT\":\n",
    "            is_not_topping = True\n",
    "            continue\n",
    "        if struct_token == \"(\" and is_not_topping:\n",
    "            not_parentheses += 1\n",
    "        if struct_token == \")\" and is_not_topping:\n",
    "            not_parentheses -= 1\n",
    "        if not_parentheses == 0:\n",
    "            is_not_topping = False\n",
    "\n",
    "        if struct_token in attribute_values:\n",
    "            curr_attr = struct_token\n",
    "            is_begin = True\n",
    "        elif struct_token not in {\"(\", \")\"} and struct_token not in execluded:\n",
    "            if curr_attr == \"NONE\":\n",
    "                continue\n",
    "            label = curr_attr\n",
    "            if is_not_topping:\n",
    "                label = \"NOT_\" + curr_attr\n",
    "            if is_begin:\n",
    "                label=\"B_\" + label\n",
    "                is_begin = False\n",
    "            else:\n",
    "                label=\"I_\" + label\n",
    "            token_label.append((struct_token, label))\n",
    "    \n",
    "    token_label_counter = 0 \n",
    "    entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17,\"I_NOT_STYLE\": 18, \"B_NOT_STYLE\": 19, \"B_NOT_QUANTITY\": 20, \"I_NOT_QUANTITY\": 21, \"NONE\": 22}\n",
    "    label_input=[]\n",
    "    label_input_nums = []\n",
    "    for in_token in input_tokens:\n",
    "        if token_label_counter >= len(token_label):\n",
    "            label_input.append((in_token,\"NONE\"))\n",
    "            label_input_nums.append(entity_to_num[\"NONE\"])\n",
    "            continue\n",
    "        if token_label[token_label_counter][0] == in_token:\n",
    "            label_input.append((in_token,token_label[token_label_counter][1]))\n",
    "            label_input_nums.append(entity_to_num[token_label[token_label_counter][1]])\n",
    "            token_label_counter += 1\n",
    "        else:\n",
    "            label_input.append((in_token,\"NONE\"))\n",
    "            label_input_nums.append(entity_to_num[\"NONE\"])\n",
    "    return label_input, label_input_nums\n",
    "            \n",
    "def label_tokens2(input_tokens, structure_tokens):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attributes = [\"PIZZAORDER\", \"DRINKORDER\", \"COMPLEX_TOPPING\"]\n",
    "    execluded = {\"ORDER\",\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\", \"QUANTITY\", \"NOT\"}\n",
    "    curr = \"NONE\"\n",
    "    # I will also keep tracking \"(\" and \")\" to know when to change the current attribute to NONE\n",
    "    parentheses =0\n",
    "    is_begin = True\n",
    "    labels_mapping = []\n",
    "    for token in structure_tokens:\n",
    "        if token in attributes:\n",
    "            curr = token\n",
    "            is_begin = True\n",
    "        elif token == \"(\":\n",
    "            parentheses += 1\n",
    "        elif token == \")\":\n",
    "            parentheses -= 1\n",
    "            if parentheses == 1:\n",
    "                curr = \"NONE\"\n",
    "        elif token not in execluded:\n",
    "            if curr == \"NONE\":\n",
    "                labels_mapping.append((token,curr))\n",
    "            elif is_begin:\n",
    "                labels_mapping.append((token, \"B_\" + curr))\n",
    "                is_begin = False\n",
    "            else:\n",
    "                labels_mapping.append((token,\"I_\" + curr))\n",
    "    labeled_output = []\n",
    "    labeled_output_nums =[]\n",
    "    labeled_output_counter = 0\n",
    "    intent_to_num = {\"I_PIZZAORDER\": 0, \"I_DRINKORDER\": 1, \"I_COMPLEX_TOPPING\": 2, \"B_PIZZAORDER\": 3, \"B_DRINKORDER\": 4, \"B_COMPLEX_TOPPING\": 5, \"NONE\": 6}\n",
    "    for token in input_tokens:\n",
    "        if labeled_output_counter >= len(labels_mapping):\n",
    "            labeled_output.append((token, \"NONE\"))\n",
    "            labeled_output_nums.append(intent_to_num[\"NONE\"])\n",
    "            continue\n",
    "        if labels_mapping[labeled_output_counter][0] == token:\n",
    "            labeled_output.append((token, labels_mapping[labeled_output_counter][1]))\n",
    "            labeled_output_nums.append(intent_to_num[labels_mapping[labeled_output_counter][1]])\n",
    "            labeled_output_counter += 1\n",
    "        else:\n",
    "            labeled_output.append((token, \"NONE\"))\n",
    "            labeled_output_nums.append(intent_to_num[\"NONE\"])\n",
    "    return labeled_output, labeled_output_nums\n",
    "\n",
    "def label_input(input_text, structure_text1, structure_text2):\n",
    "    \"\"\"\n",
    "    It is a similar function to the previous one, but it is used for adding another layer for the input\n",
    "    which is the preprocessing of the input text and then tokenizing it.\n",
    "\n",
    "    Args:\n",
    "        input_text: The raw input text.\n",
    "        structure_text1: The structured text containing attributes and their values. (train.TOP-DECOUPLED)\n",
    "        structure_text2: The structured text containing attributes and their values. (train.TOP)\n",
    "    \n",
    "    Returns:\n",
    "        2 lists of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_string(input_text)\n",
    "    input_tokens = tokenize_string(cleaned_text)\n",
    "    structure1_tokens = tokenize_string(structure_text1)\n",
    "    labeled_output1, _ = label_tokens1(input_tokens, structure1_tokens)\n",
    "    structure2_tokens = tokenize_string(structure_text2)\n",
    "    labeled_output2 , _= label_tokens2(input_tokens, structure2_tokens)\n",
    "    return labeled_output1, labeled_output2\n",
    "\n",
    "def label_complete_input (input_list, structure_text1_list, structure_text2_list):\n",
    "    \"\"\"\n",
    "    It is a similar function to the previous one, but it takes inputs as lists of tokens instead of strings.\n",
    "\n",
    "    Args:\n",
    "        input_text: The raw input text.\n",
    "        structure_text1: The structured text containing attributes and their values. (train.TOP-DECOUPLED)\n",
    "        structure_text2: The structured text containing attributes and their values. (train.TOP)\n",
    "    \n",
    "    Returns:\n",
    "        2 lists of tuples where each token in the input text is paired with its corresponding label.\n",
    "        1 list of tokens for input text.\n",
    "    \"\"\"\n",
    "    labeled_output1 = []\n",
    "    labeled_output2 = []\n",
    "    list_of_tokens = []\n",
    "    for text, struct1, struct2 in zip(input_list, structure_text1_list, structure_text2_list):\n",
    "        cleaned_text = clean_string(text)\n",
    "        input_tokens = tokenize_string(cleaned_text)\n",
    "        list_of_tokens.append(input_tokens)\n",
    "        structure1_tokens = tokenize_string(struct1)\n",
    "        _, labels = label_tokens1(input_tokens, structure1_tokens)\n",
    "        labeled_output1.append(labels)\n",
    "        structure2_tokens = tokenize_string(struct2)\n",
    "        _, labels = label_tokens2(input_tokens, structure2_tokens)\n",
    "        labeled_output2.append(labels)\n",
    "    return labeled_output1, labeled_output2, list_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------1------------------\n",
      "1  [('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('large', 'B_SIZE'), ('vegetarian', 'B_STYLE'), ('pizza', 'NONE')]\n",
      "2  [('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_PIZZAORDER'), ('large', 'I_PIZZAORDER'), ('vegetarian', 'I_PIZZAORDER'), ('pizza', 'I_PIZZAORDER')]\n",
      "------------------2------------------\n",
      "1  [('a', 'B_NUMBER'), ('20', 'B_VOLUME'), ('fl', 'I_VOLUME'), ('ounce', 'I_VOLUME'), ('cherry', 'B_DRINKTYPE'), ('coke', 'I_DRINKTYPE'), ('bottle', 'B_CONTAINERTYPE')]\n",
      "2  [('a', 'B_DRINKORDER'), ('20', 'I_DRINKORDER'), ('fl', 'I_DRINKORDER'), ('ounce', 'I_DRINKORDER'), ('cherry', 'I_DRINKORDER'), ('coke', 'I_DRINKORDER'), ('bottle', 'I_DRINKORDER')]\n",
      "------------------3------------------\n",
      "1  [('four', 'B_NUMBER'), ('pizzas', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('also', 'NONE'), ('three', 'B_NUMBER'), ('cans', 'B_CONTAINERTYPE'), ('of', 'NONE'), ('ice', 'B_DRINKTYPE'), ('tea', 'I_DRINKTYPE'), ('and', 'NONE'), ('three', 'B_NUMBER'), ('regular', 'B_SIZE'), ('san', 'B_DRINKTYPE'), ('pellegrinos', 'I_DRINKTYPE')]\n",
      "2  [('four', 'B_PIZZAORDER'), ('pizzas', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'NONE'), ('also', 'NONE'), ('three', 'B_DRINKORDER'), ('cans', 'I_DRINKORDER'), ('of', 'I_DRINKORDER'), ('ice', 'I_DRINKORDER'), ('tea', 'I_DRINKORDER'), ('and', 'NONE'), ('three', 'B_DRINKORDER'), ('regular', 'I_DRINKORDER'), ('san', 'I_DRINKORDER'), ('pellegrinos', 'I_DRINKORDER')]\n",
      "------------------4------------------\n",
      "1  [('i', 'NONE'), ('want', 'NONE'), ('one', 'B_NUMBER'), ('personal', 'B_SIZE'), ('-', 'I_SIZE'), ('size', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('any', 'NONE'), ('carrots', 'B_NOT_TOPPING')]\n",
      "2  [('i', 'NONE'), ('want', 'NONE'), ('one', 'B_PIZZAORDER'), ('personal', 'I_PIZZAORDER'), ('-', 'I_PIZZAORDER'), ('size', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('any', 'I_PIZZAORDER'), ('carrots', 'I_PIZZAORDER')]\n",
      "------------------5------------------\n",
      "1  [('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_NUMBER'), ('high', 'B_STYLE'), ('rise', 'I_STYLE'), ('dough', 'I_STYLE'), ('pie', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('a', 'B_QUANTITY'), ('lot', 'I_QUANTITY'), ('of', 'I_QUANTITY'), ('meatball', 'B_TOPPING')]\n",
      "2  [('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_PIZZAORDER'), ('high', 'I_PIZZAORDER'), ('rise', 'I_PIZZAORDER'), ('dough', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'I_PIZZAORDER'), ('a', 'B_COMPLEX_TOPPING'), ('lot', 'I_COMPLEX_TOPPING'), ('of', 'I_COMPLEX_TOPPING'), ('meatball', 'I_COMPLEX_TOPPING')]\n",
      "------------------6------------------\n",
      "1  [('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('lunch', 'B_SIZE'), ('-', 'I_SIZE'), ('sized', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('alfredo', 'B_NOT_TOPPING'), ('chicken', 'I_NOT_TOPPING')]\n",
      "2  [('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_PIZZAORDER'), ('lunch', 'I_PIZZAORDER'), ('-', 'I_PIZZAORDER'), ('sized', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('alfredo', 'I_PIZZAORDER'), ('chicken', 'I_PIZZAORDER')]\n",
      "------------------7------------------\n",
      "1  [('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('lunch', 'B_SIZE'), ('-', 'I_SIZE'), ('sized', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('alfredo', 'B_NOT_TOPPING'), ('chicken', 'I_NOT_TOPPING'), ('or', 'NONE'), ('beef', 'B_NOT_TOPPING')]\n",
      "2  [('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_PIZZAORDER'), ('lunch', 'I_PIZZAORDER'), ('-', 'I_PIZZAORDER'), ('sized', 'I_PIZZAORDER'), ('pie', 'I_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('alfredo', 'I_PIZZAORDER'), ('chicken', 'I_PIZZAORDER'), ('or', 'I_PIZZAORDER'), ('beef', 'I_PIZZAORDER')]\n",
      "------------------8------------------\n",
      "1  [('pie', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('with', 'NONE'), ('not', 'B_QUANTITY'), ('much', 'I_QUANTITY'), ('parmesan', 'B_TOPPING'), ('cheese', 'I_TOPPING')]\n",
      "2  [('pie', 'B_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('not', 'B_COMPLEX_TOPPING'), ('much', 'I_COMPLEX_TOPPING'), ('parmesan', 'I_COMPLEX_TOPPING'), ('cheese', 'I_COMPLEX_TOPPING')]\n",
      "------------------9------------------\n",
      "1  [('pie', 'NONE'), ('without', 'NONE'), ('american', 'B_NOT_TOPPING'), ('cheese', 'I_NOT_TOPPING'), ('and', 'NONE'), ('with', 'NONE'), ('parmesan', 'B_TOPPING'), ('cheese', 'I_TOPPING')]\n",
      "2  [('pie', 'B_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('parmesan', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER')]\n",
      "------------------10------------------\n",
      "1  [('i', 'NONE'), ('want', 'NONE'), ('three', 'B_NUMBER'), ('pies', 'NONE'), ('with', 'NONE'), ('pesto', 'B_TOPPING'), ('sauce', 'I_TOPPING'), ('and', 'NONE'), ('without', 'NONE'), ('any', 'NONE'), ('shrimps', 'B_NOT_TOPPING')]\n",
      "2  [('i', 'NONE'), ('want', 'NONE'), ('three', 'B_PIZZAORDER'), ('pies', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('pesto', 'I_PIZZAORDER'), ('sauce', 'I_PIZZAORDER'), ('and', 'I_PIZZAORDER'), ('without', 'I_PIZZAORDER'), ('any', 'I_PIZZAORDER'), ('shrimps', 'I_PIZZAORDER')]\n",
      "------------------10------------------\n",
      "1  [('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_NUMBER'), ('party', 'B_SIZE'), ('sized', 'I_SIZE'), ('high', 'B_STYLE'), ('rise', 'I_STYLE'), ('dough', 'I_STYLE'), ('pizza', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('a', 'B_QUANTITY'), ('lot', 'I_QUANTITY'), ('of', 'I_QUANTITY'), ('peperonni', 'B_TOPPING')]\n",
      "2  [('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_PIZZAORDER'), ('party', 'I_PIZZAORDER'), ('sized', 'I_PIZZAORDER'), ('high', 'I_PIZZAORDER'), ('rise', 'I_PIZZAORDER'), ('dough', 'I_PIZZAORDER'), ('pizza', 'I_PIZZAORDER'), ('with', 'I_PIZZAORDER'), ('american', 'I_PIZZAORDER'), ('cheese', 'I_PIZZAORDER'), ('and', 'I_PIZZAORDER'), ('a', 'B_COMPLEX_TOPPING'), ('lot', 'I_COMPLEX_TOPPING'), ('of', 'I_COMPLEX_TOPPING'), ('peperonni', 'I_COMPLEX_TOPPING')]\n"
     ]
    }
   ],
   "source": [
    "out1,out2 = label_input(\"i'd like a large vegetarian pizza\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (STYLE vegetarian ) ) )\",\"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE large ) (STYLE vegetarian ) pizza ) )\")\n",
    "print(\"------------------1------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"a 20 fl ounce cherry coke bottle\", \"(ORDER (DRINKORDER (NUMBER a ) (VOLUME 20 fl ounce ) (DRINKTYPE cherry coke ) (CONTAINERTYPE bottle ) ) )\", \"(ORDER (DRINKORDER (NUMBER a ) (VOLUME 20 fl ounce ) (DRINKTYPE cherry coke ) (CONTAINERTYPE bottle ) ) )\")\n",
    "print(\"------------------2------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"four pizzas with american cheese and also three cans of ice tea and three regular san pellegrinos\", \"(ORDER (PIZZAORDER (NUMBER four ) (TOPPING american cheese ) ) (DRINKORDER (NUMBER three ) (CONTAINERTYPE cans ) (DRINKTYPE ice tea ) ) (DRINKORDER (NUMBER three ) (SIZE regular ) (DRINKTYPE san pellegrinos ) ) )\", \"(ORDER (PIZZAORDER (NUMBER four ) pizzas with (TOPPING american cheese ) ) and also (DRINKORDER (NUMBER three ) (CONTAINERTYPE cans ) of (DRINKTYPE ice tea ) ) and (DRINKORDER (NUMBER three ) (SIZE regular ) (DRINKTYPE san pellegrinos ) ) )\")\n",
    "print(\"------------------3------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"i want one personal - size pie without any carrots\", \"(ORDER (PIZZAORDER (NUMBER one ) (SIZE personal - size ) (NOT (TOPPING carrots ) ) ) )\", \"(ORDER i want (PIZZAORDER (NUMBER one ) (SIZE personal - size ) pie without any (NOT (TOPPING carrots ) ) ) )\")\n",
    "print(\"------------------4------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"can i have one high rise dough pie with american cheese and a lot of meatball\", \"(ORDER (PIZZAORDER (NUMBER one ) (STYLE high rise dough ) (TOPPING american cheese ) (COMPLEX_TOPPING ( QUANTITY a lot of ) (TOPPING meatball ) ) ) )\", \"(ORDER can i have (PIZZAORDER (NUMBER one ) (STYLE high rise dough ) pie with (TOPPING american cheese ) and (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING meatball ) ) ) )\") \n",
    "print(\"------------------5------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"i'd like a lunch - sized pie without alfredo chicken\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) (NOT (TOPPING alfredo chicken ) ) ) )\", \"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) pie without (NOT (TOPPING alfredo chicken ) ) ) )\") \n",
    "print(\"------------------6------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"i'd like a lunch - sized pie without alfredo chicken or beef\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) (NOT (TOPPING alfredo chicken ) )(NOT (TOPPING beef ) ) ) )\", \"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) pie without (NOT (TOPPING alfredo chicken ) ) or (NOT (TOPPING beef ) ) ) )\") \n",
    "print(\"------------------7------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"pie with american cheese and with not much parmesan cheese\", \"(ORDER (PIZZAORDER (TOPPING american cheese ) (COMPLEX_TOPPING (QUANTITY not much ) (TOPPING parmesan cheese ) ) ) )\", \"(ORDER (PIZZAORDER pie with (TOPPING american cheese ) and with (COMPLEX_TOPPING (QUANTITY not much ) (TOPPING parmesan cheese ) ) ) )\") \n",
    "print(\"------------------8------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"pie without american cheese and with parmesan cheese\", \"(ORDER (PIZZAORDER (NOT(TOPPING american cheese )) (TOPPING parmesan cheese ) ) )\", \"(ORDER (PIZZAORDER pie without (NOT(TOPPING american cheese )) and with (TOPPING parmesan cheese ) ) )\") \n",
    "print(\"------------------9------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"i want three pies with pesto sauce and without any shrimps\", \"(ORDER (PIZZAORDER (NUMBER three ) (TOPPING pesto sauce ) (NOT (TOPPING shrimps ) ) ) )\", \"(ORDER i want (PIZZAORDER (NUMBER three ) pies with (TOPPING pesto sauce ) and without any (NOT (TOPPING shrimps ) ) ) )\") \n",
    "print(\"------------------10------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)\n",
    "out1, out2 = label_input(\"can i have one party sized high rise dough pizza with american cheese and a lot of peperonni\", \"(ORDER (PIZZAORDER (NUMBER one ) (SIZE party sized ) (STYLE high rise dough ) (TOPPING american cheese ) (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING peperonni ) ) ) )\", \"(ORDER can i have (PIZZAORDER (NUMBER one ) (SIZE party sized ) (STYLE high rise dough ) pizza with (TOPPING american cheese ) and (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING peperonni ) ) ) )\") \n",
    "print(\"------------------10------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "def fix_json_file(path):\n",
    "    \"\"\"\n",
    "    Fixes a corrupted JSON file by formatting it properly.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the corrupted JSON file.\n",
    "\n",
    "    Returns:\n",
    "        None. Writes a corrected version of the JSON file to disk.\n",
    "    \"\"\"\n",
    "    fixed_file = open(\"../fixed_train.json\", \"w\")\n",
    "    fixed_file.write(\"[\\n\")\n",
    "    with open(path, \"r\") as file:\n",
    "        for line in file:\n",
    "            fixed_file.write(line[:-1] + \",\\n\")\n",
    "    fixed_file.seek(fixed_file.tell() - 3)\n",
    "    fixed_file.truncate()\n",
    "    fixed_file.write(\"]\")\n",
    "    fixed_file.close()\n",
    "fix_json_file(\"../data/fixed_PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and loads its content into a Python object.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Parsed JSON data as a Python object.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        part_of_data = data[:100]\n",
    "    return part_of_data\n",
    "data = read_data(\"../data/fixed_PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_corpus_from_json(data):\n",
    "    \"\"\"\n",
    "    Builds a training corpus from a JSON-like dataset.\n",
    "    Extracts the \"train.SRC\" field from each item in the dataset.\n",
    "\n",
    "    Args:\n",
    "        data: List of dictionaries, where each dictionary contains a \"train.SRC\" key.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the training corpus.\n",
    "    \"\"\"\n",
    "    src, top, decoupled = [], [], []\n",
    "    for d in data:\n",
    "        src.append(d[\"train.SRC\"])\n",
    "        top.append(d[\"train.TOP\"])\n",
    "        decoupled.append(d[\"train.TOP-DECOUPLED\"])\n",
    "    return src, top, decoupled\n",
    "src, top, decoupled = build_train_corpus_from_json(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can i have a large bbq pulled pork\n",
      "(ORDER can i have (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\n",
      "(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\n"
     ]
    }
   ],
   "source": [
    "print(src[0])\n",
    "print(top[0])\n",
    "print(decoupled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18, 18, 18, 8, 9, 10, 2, 2]\n",
      "[6, 6, 6, 3, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "entites, intents = label_complete_input(src, top, decoupled)\n",
    "# assert that the length of the entities and intents is the same as the length of the src for each one\n",
    "print(entites[0])\n",
    "print(intents[0])\n",
    "for src_, ent_, intent_ in zip(src, entites, intents):\n",
    "    assert len(tokenize_string(src_)) == len(ent_) == len(intent_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText  # For Word2Vec model\n",
    "import gensim  # General Gensim utilities\n",
    "import nltk  # For tokenization and natural language processing\n",
    "import json  # For handling JSON files\n",
    "# from transformers import BertTokenizer, BertModel  # BERT tokenizer and model\n",
    "# import torch  # For PyTorch tensors and operations\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters for the Word2Vec model\n",
    "VECTOR_SIZE = 50  # Size of word vectors\n",
    "WINDOW_SIZE = 5  # Context window size\n",
    "THREADS = 4  # Number of threads to use for training\n",
    "CUTOFF_FREQ = 1  # Minimum frequency for a word to be included in vocabulary\n",
    "EPOCHS = 100  # Number of training epochs\n",
    "\n",
    "def list_of_lists(sentences):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into a list of tokenized sentences.\n",
    "    Each sentence is split into individual words.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of strings where each string is a sentence.\n",
    "\n",
    "    Returns:\n",
    "        List of lists where each inner list contains tokens of a sentence.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sentence))\n",
    "    return tokenized_sentences\n",
    "\n",
    "def train_gensim_w2v_model(corpus):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the given corpus of sentences.\n",
    "\n",
    "    Args:\n",
    "        corpus: List of sentences (strings).\n",
    "\n",
    "    Returns:\n",
    "        A trained Gensim Word2Vec model.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = list_of_lists(corpus)\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_sentences,\n",
    "        vector_size=VECTOR_SIZE,\n",
    "        window=WINDOW_SIZE,\n",
    "        min_count=CUTOFF_FREQ,\n",
    "        workers=THREADS,\n",
    "    )\n",
    "    model.build_vocab(tokenized_sentences)\n",
    "    model.train(\n",
    "        corpus_iterable=tokenized_sentences,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=EPOCHS,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def embed_gensim(model, word):\n",
    "    \"\"\"\n",
    "    Retrieves the word embedding for a given word using a trained Gensim model.\n",
    "    Works for both w2v and fastext.\n",
    "    Args:\n",
    "        model: Trained Gensim Word2Vec model.\n",
    "        word: Word to retrieve the embedding for.\n",
    "\n",
    "    Returns:\n",
    "        Word embedding as a vector.\n",
    "    \"\"\"\n",
    "    return model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.load('./Models/model_100k.pth', weights_only=False, map_location=torch.device('cpu'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17,\"I_NOT_STYLE\": 18, \"B_NOT_STYLE\": 19, \"B_NOT_QUANTITY\": 20, \"I_NOT_QUANTITY\": 21, \"NONE\": 22}\n",
    "intent_to_num = {\"I_PIZZAORDER\": 0, \"I_DRINKORDER\": 1, \"I_COMPLEX_TOPPING\": 2, \"B_PIZZAORDER\": 3, \"B_DRINKORDER\": 4, \"B_COMPLEX_TOPPING\": 5, \"NONE\": 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------10------------------\n",
      "1  [[18, 18, 18, 8, 9, 1, 11, 3, 3, 18, 18, 10, 2, 18, 15, 7, 7, 10]]\n",
      "2  [[6, 6, 6, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 2, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "out1, out2, _ = label_complete_input([\"can i have one party sized high rise dough pizza with american cheese and a lot of peperonni\"], [\"(ORDER (PIZZAORDER (NUMBER one ) (SIZE party sized ) (STYLE high rise dough ) (TOPPING american cheese ) (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING peperonni ) ) ) )\"], [\"(ORDER can i have (PIZZAORDER (NUMBER one ) (SIZE party sized ) (STYLE high rise dough ) pizza with (TOPPING american cheese ) and (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING peperonni ) ) ) )\"]) \n",
    "print(\"------------------10------------------\")\n",
    "print(\"1 \",out1)\n",
    "print(\"2 \",out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_tokens_dev(input_tokens, structure_tokens):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values as tokens.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attribute_values = {\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\", \"QUANTITY\"}\n",
    "    execluded = [\"PIZZAORDER\", \"DRINKORDER\", \"COMPLEX_TOPPING\"]\n",
    "    token_label = []\n",
    "    curr_attr = \"NONE\"\n",
    "    is_not_topping = False\n",
    "    not_parentheses = 0\n",
    "    is_begin = True\n",
    "    for struct_token in structure_tokens:\n",
    "        if struct_token == \"NOT\":\n",
    "            is_not_topping = True\n",
    "            continue\n",
    "        if struct_token == \"(\" and is_not_topping:\n",
    "            not_parentheses += 1\n",
    "        if struct_token == \")\" and is_not_topping:\n",
    "            not_parentheses -= 1\n",
    "        elif struct_token == \")\" :\n",
    "            curr_attr = \"NONE\"\n",
    "            is_begin = True\n",
    "\n",
    "        if not_parentheses == 0:\n",
    "            is_not_topping = False\n",
    "\n",
    "        if struct_token in attribute_values:\n",
    "            curr_attr = struct_token\n",
    "            is_begin = True\n",
    "        elif struct_token not in {\"(\", \")\"} and struct_token not in execluded:\n",
    "            if curr_attr == \"NONE\":\n",
    "                continue\n",
    "            label = curr_attr\n",
    "            if is_not_topping:\n",
    "                label = \"NOT_\" + curr_attr\n",
    "            if is_begin:\n",
    "                label=\"B_\" + label\n",
    "                is_begin = False\n",
    "            else:\n",
    "                label=\"I_\" + label\n",
    "            token_label.append((struct_token, label))\n",
    "    \n",
    "    token_label_counter = 0 \n",
    "    entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17,\"I_NOT_STYLE\": 18, \"B_NOT_STYLE\": 19, \"B_NOT_QUANTITY\": 20, \"I_NOT_QUANTITY\": 21, \"NONE\": 22}\n",
    "    label_input=[]  \n",
    "    label_input_nums = []\n",
    "    for in_token in input_tokens:\n",
    "        if token_label_counter >= len(token_label):\n",
    "            label_input.append((in_token,\"NONE\"))\n",
    "            label_input_nums.append(entity_to_num[\"NONE\"])\n",
    "            continue\n",
    "        if token_label[token_label_counter][0] == in_token:\n",
    "            label_input.append((in_token,token_label[token_label_counter][1]))\n",
    "            label_input_nums.append(entity_to_num[token_label[token_label_counter][1]])\n",
    "            token_label_counter += 1\n",
    "        else:\n",
    "            label_input.append((in_token,\"NONE\"))\n",
    "            label_input_nums.append(entity_to_num[\"NONE\"])\n",
    "    return label_input, label_input_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------1------------------\n",
      "1  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('large', 'B_SIZE'), ('vegetarian', 'B_STYLE'), ('pizza', 'NONE')], [22, 22, 22, 8, 9, 11, 22])\n",
      "------------------2------------------\n",
      "1  ([('a', 'B_NUMBER'), ('20', 'B_VOLUME'), ('fl', 'I_VOLUME'), ('ounce', 'I_VOLUME'), ('cherry', 'B_DRINKTYPE'), ('coke', 'I_DRINKTYPE'), ('bottle', 'B_CONTAINERTYPE')], [8, 14, 6, 6, 12, 4, 13])\n",
      "------------------3------------------\n",
      "1  ([('four', 'B_NUMBER'), ('pizzas', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('also', 'NONE'), ('three', 'B_NUMBER'), ('cans', 'B_CONTAINERTYPE'), ('of', 'NONE'), ('ice', 'B_DRINKTYPE'), ('tea', 'I_DRINKTYPE'), ('and', 'NONE'), ('three', 'B_NUMBER'), ('regular', 'B_SIZE'), ('san', 'B_DRINKTYPE'), ('pellegrinos', 'I_DRINKTYPE')], [8, 22, 22, 10, 2, 22, 22, 8, 13, 22, 12, 4, 22, 8, 9, 12, 4])\n",
      "------------------4------------------\n",
      "1  ([('i', 'NONE'), ('want', 'NONE'), ('one', 'B_NUMBER'), ('personal', 'B_SIZE'), ('-', 'I_SIZE'), ('size', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('any', 'NONE'), ('carrots', 'B_NOT_TOPPING')], [22, 22, 8, 9, 1, 1, 22, 22, 22, 17])\n",
      "------------------5------------------\n",
      "1  ([('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_NUMBER'), ('high', 'B_STYLE'), ('rise', 'I_STYLE'), ('dough', 'I_STYLE'), ('pie', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('a', 'B_QUANTITY'), ('lot', 'I_QUANTITY'), ('of', 'I_QUANTITY'), ('meatball', 'B_TOPPING')], [22, 22, 22, 8, 11, 3, 3, 22, 22, 10, 2, 22, 15, 7, 7, 10])\n",
      "------------------6------------------\n",
      "1  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('lunch', 'B_SIZE'), ('-', 'I_SIZE'), ('sized', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('alfredo', 'B_NOT_TOPPING'), ('chicken', 'I_NOT_TOPPING')], [22, 22, 22, 8, 9, 1, 1, 22, 22, 17, 16])\n",
      "------------------7------------------\n",
      "1  ([('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'B_NUMBER'), ('lunch', 'B_SIZE'), ('-', 'I_SIZE'), ('sized', 'I_SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('alfredo', 'B_NOT_TOPPING'), ('chicken', 'I_NOT_TOPPING'), ('or', 'NONE'), ('beef', 'B_NOT_TOPPING')], [22, 22, 22, 8, 9, 1, 1, 22, 22, 17, 16, 22, 17])\n",
      "------------------8------------------\n",
      "1  ([('pie', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('with', 'NONE'), ('not', 'B_QUANTITY'), ('much', 'I_QUANTITY'), ('parmesan', 'B_TOPPING'), ('cheese', 'I_TOPPING')], [22, 22, 10, 2, 22, 22, 15, 7, 10, 2])\n",
      "------------------9------------------\n",
      "1  ([('pie', 'NONE'), ('without', 'NONE'), ('american', 'B_NOT_TOPPING'), ('cheese', 'I_NOT_TOPPING'), ('and', 'NONE'), ('with', 'NONE'), ('parmesan', 'B_TOPPING'), ('cheese', 'I_TOPPING')], [22, 22, 17, 16, 22, 22, 10, 2])\n",
      "------------------10------------------\n",
      "1  ([('i', 'NONE'), ('want', 'NONE'), ('three', 'B_NUMBER'), ('pies', 'NONE'), ('with', 'NONE'), ('pesto', 'B_TOPPING'), ('sauce', 'I_TOPPING'), ('and', 'NONE'), ('without', 'NONE'), ('any', 'NONE'), ('shrimps', 'B_NOT_TOPPING')], [22, 22, 8, 22, 22, 10, 2, 22, 22, 22, 17])\n",
      "------------------10------------------\n",
      "1  ([('can', 'NONE'), ('i', 'NONE'), ('have', 'NONE'), ('one', 'B_NUMBER'), ('party', 'B_SIZE'), ('sized', 'I_SIZE'), ('high', 'B_STYLE'), ('rise', 'I_STYLE'), ('dough', 'I_STYLE'), ('pizza', 'NONE'), ('with', 'NONE'), ('american', 'B_TOPPING'), ('cheese', 'I_TOPPING'), ('and', 'NONE'), ('a', 'B_QUANTITY'), ('lot', 'I_QUANTITY'), ('of', 'I_QUANTITY'), ('peperonni', 'B_TOPPING')], [22, 22, 22, 8, 9, 1, 11, 3, 3, 22, 22, 10, 2, 22, 15, 7, 7, 10])\n",
      "------------------11------------------\n",
      "1  ([('get', 'NONE'), ('me', 'NONE'), ('a', 'B_NUMBER'), ('large', 'B_SIZE'), ('ham', 'B_TOPPING'), ('and', 'NONE'), ('pepper', 'B_TOPPING'), ('pie', 'NONE'), ('without', 'NONE'), ('the', 'NONE'), ('thin', 'B_NOT_STYLE'), ('crust', 'I_NOT_STYLE')], [22, 22, 8, 9, 10, 22, 10, 22, 22, 22, 19, 18])\n",
      "------------------11------------------\n",
      "1  ([('i', 'NONE'), ('want', 'NONE'), ('a', 'B_NUMBER'), ('pizza', 'NONE'), ('with', 'NONE'), ('sausage', 'B_TOPPING'), ('bacon', 'B_TOPPING'), ('and', 'NONE'), ('no', 'NONE'), ('extra', 'B_NOT_QUANTITY'), ('cheese', 'B_NOT_TOPPING')], [22, 22, 8, 22, 22, 10, 10, 22, 22, 20, 17])\n"
     ]
    }
   ],
   "source": [
    "out1= label_tokens_dev(tokenize_string(\"i'd like a large vegetarian pizza\"),tokenize_string(\"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE large ) (STYLE vegetarian ) pizza ) )\"))\n",
    "print(\"------------------1------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1= label_tokens_dev(tokenize_string(\"a 20 fl ounce cherry coke bottle\"), tokenize_string(\"(ORDER (DRINKORDER (NUMBER a ) (VOLUME 20 fl ounce ) (DRINKTYPE cherry coke ) (CONTAINERTYPE bottle ) ) )\"))\n",
    "print(\"------------------2------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"four pizzas with american cheese and also three cans of ice tea and three regular san pellegrinos\"), tokenize_string(\"(ORDER (PIZZAORDER (NUMBER four ) pizzas with (TOPPING american cheese ) ) and also (DRINKORDER (NUMBER three ) (CONTAINERTYPE cans ) of (DRINKTYPE ice tea ) ) and (DRINKORDER (NUMBER three ) (SIZE regular ) (DRINKTYPE san pellegrinos ) ) )\"))\n",
    "print(\"------------------3------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"i want one personal - size pie without any carrots\"), tokenize_string(\"(ORDER i want (PIZZAORDER (NUMBER one ) (SIZE personal - size ) pie without any (NOT (TOPPING carrots ) ) ) )\"))\n",
    "print(\"------------------4------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"can i have one high rise dough pie with american cheese and a lot of meatball\"),  tokenize_string(\"(ORDER can i have (PIZZAORDER (NUMBER one ) (STYLE high rise dough ) pie with (TOPPING american cheese ) and (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING meatball ) ) ) )\")) \n",
    "print(\"------------------5------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"i'd like a lunch - sized pie without alfredo chicken\"),  tokenize_string(\"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) pie without (NOT (TOPPING alfredo chicken ) ) ) )\")) \n",
    "print(\"------------------6------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"i'd like a lunch - sized pie without alfredo chicken or beef\"),  tokenize_string(\"(ORDER i'd like (PIZZAORDER (NUMBER a ) (SIZE lunch - sized ) pie without (NOT (TOPPING alfredo chicken ) ) or (NOT (TOPPING beef ) ) ) )\")) \n",
    "print(\"------------------7------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"pie with american cheese and with not much parmesan cheese\"), tokenize_string(\"(ORDER (PIZZAORDER pie with (TOPPING american cheese ) and with (COMPLEX_TOPPING (QUANTITY not much ) (TOPPING parmesan cheese ) ) ) )\")) \n",
    "print(\"------------------8------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"pie without american cheese and with parmesan cheese\"), tokenize_string( \"(ORDER (PIZZAORDER pie without (NOT(TOPPING american cheese )) and with (TOPPING parmesan cheese ) ) )\")) \n",
    "print(\"------------------9------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"i want three pies with pesto sauce and without any shrimps\"),  tokenize_string(\"(ORDER i want (PIZZAORDER (NUMBER three ) pies with (TOPPING pesto sauce ) and without any (NOT (TOPPING shrimps ) ) ) )\")) \n",
    "print(\"------------------10------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"can i have one party sized high rise dough pizza with american cheese and a lot of peperonni\"), tokenize_string(\"(ORDER can i have (PIZZAORDER (NUMBER one ) (SIZE party sized ) (STYLE high rise dough ) pizza with (TOPPING american cheese ) and (COMPLEX_TOPPING (QUANTITY a lot of ) (TOPPING peperonni ) ) ) )\")) \n",
    "print(\"------------------10------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"get me a large ham and pepper pie without the thin crust\"), tokenize_string(\"(ORDER get me (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING ham ) and (TOPPING pepper ) pie without the (NOT (STYLE thin crust ) ) ) )\")) \n",
    "print(\"------------------11------------------\")\n",
    "print(\"1 \",out1)\n",
    "out1 = label_tokens_dev(tokenize_string(\"i want a pizza with sausage bacon and no extra cheese\"), tokenize_string(\"(ORDER i want (PIZZAORDER (NUMBER a ) pizza with (TOPPING sausage ) (TOPPING bacon ) and no (NOT (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING cheese ) ) ) ) )\")) \n",
    "print(\"------------------11------------------\")\n",
    "print(\"1 \",out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
