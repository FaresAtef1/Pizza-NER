{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook we will do some preprocessing on the data and tokenization\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"\n",
    "    Cleans the input string by removing special characters, and unnecessary punctuation.\n",
    "\n",
    "    Args:\n",
    "        input_string: The string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned string.\n",
    "    \"\"\"\n",
    "    # Remove special characters and unnecessary punctuation\n",
    "    cleaned_string = re.sub(r\"[^\\w\\s']\", \"\", input_string)  # Keeps only alphanumeric characters and spaces and apostrophes\n",
    "    # Convert string to lowercase for uniform processing\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    # Remove extra whitespace\n",
    "    cleaned_string = re.sub(r\"\\s+\", \" \", cleaned_string).strip()\n",
    "    return cleaned_string\n",
    "\n",
    "def tokenize_string(input_string):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string into tokens.\n",
    "\n",
    "    Args:\n",
    "        input_string: The string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    # Tokenize the string into words\n",
    "    tokens = nltk.word_tokenize(input_string)\n",
    "    return tokens\n",
    "\n",
    "def label_tokens(input_tokens, structure_text):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attribute_values = {\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\"}\n",
    "    structure_map = {}\n",
    "    for attribute in attribute_values:\n",
    "        # Match the attribute and its value in the structure text\n",
    "        pattern = r\"\\(\\s*\"+ attribute + r\"\\s+([^\\)]*)\\s*\\)\"\n",
    "        match = re.search(pattern, structure_text)\n",
    "        if match:\n",
    "            value = match.group(1).strip()\n",
    "            structure_map[value] = attribute\n",
    "    labeled_output = []\n",
    "    for token in input_tokens:\n",
    "        label = \"NONE\"\n",
    "        if token in structure_map:\n",
    "            label = structure_map[token]\n",
    "        labeled_output.append((token, label))\n",
    "    return labeled_output\n",
    "\n",
    "def label_input(input_text, structure_text):\n",
    "    \"\"\"\n",
    "    It is a similar function to the previous one, but it is used for adding another layer for the input\n",
    "    which is the preprocessing of the input text and then tokenizing it.\n",
    "\n",
    "    Args:\n",
    "        input_text: The raw input text.\n",
    "        structure_text: The structured text containing attributes and their values.\n",
    "    \n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_string(input_text)\n",
    "    tokens = tokenize_string(cleaned_text)\n",
    "    labeled_output = label_tokens(tokens, structure_text)    \n",
    "    return labeled_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'NONE'), (\"'d\", 'NONE'), ('like', 'NONE'), ('a', 'NUMBER'), ('large', 'SIZE'), ('vegetarian', 'STYLE'), ('pizza', 'NONE')]\n",
      "[('a', 'NUMBER'), ('20', 'NONE'), ('fl', 'NONE'), ('ounce', 'NONE'), ('cherry', 'NONE'), ('coke', 'NONE'), ('bottle', 'CONTAINERTYPE')]\n",
      "[('three', 'NUMBER'), ('threeliter', 'NONE'), ('san', 'NONE'), ('pellegrinos', 'NONE'), ('in', 'NONE'), ('cans', 'NONE')]\n",
      "[('i', 'NONE'), ('want', 'NONE'), ('a', 'NUMBER'), ('personal', 'SIZE'), ('pie', 'NONE'), ('without', 'NONE'), ('hams', 'TOPPING')]\n"
     ]
    }
   ],
   "source": [
    "out1 = label_input(\"i'd like a large vegetarian pizza\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (STYLE vegetarian ) ) )\")\n",
    "print(out1)\n",
    "out2 = label_input(\"a 20 fl ounce cherry coke bottle\", \"(ORDER (DRINKORDER (NUMBER a ) (VOLUME 20 fl ounce ) (DRINKTYPE cherry coke ) (CONTAINERTYPE bottle ) ) )\")\n",
    "print(out2)\n",
    "out3 = label_input(\"three three-liter san pellegrinos in cans\", \"(ORDER (DRINKORDER (NUMBER three ) (VOLUME three-liter ) (DRINKTYPE san pellegrinos ) (CONTAINERTYPE in cans ) ) )\")\n",
    "print(out3)\n",
    "out4 = label_input(\"i want a personal pie without hams\", \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE personal ) (NOT (TOPPING hams ) ) ) )\")\n",
    "print(out4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
