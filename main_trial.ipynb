{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# this part defines functions used in formatting input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this notebook we will do some preprocessing on the data and tokenization\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "def clean_string(input_string):\n",
    "    \"\"\"\n",
    "    Cleans the input string by removing special characters, and unnecessary punctuation.\n",
    "\n",
    "    Args:\n",
    "        input_string: The string to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned string.\n",
    "    \"\"\"\n",
    "    # Remove special characters and unnecessary punctuation\n",
    "    # TODO: Add more special characters as needed to be excluded\n",
    "    cleaned_string = re.sub(r\"[^\\w\\s'-]\", \"\", input_string)  # Keeps only alphanumeric characters and spaces and apostrophes and hyphens\n",
    "    cleaned_string = cleaned_string.lower()\n",
    "    # Remove extra whitespace\n",
    "    cleaned_string = re.sub(r\"\\s+\", \" \", cleaned_string).strip()\n",
    "    return cleaned_string\n",
    "\n",
    "def tokenize_string(input_string):\n",
    "    \"\"\"\n",
    "    Tokenizes the input string into tokens.\n",
    "    \n",
    "    Args:\n",
    "        input_string: The string to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        A list of tokens.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(input_string)\n",
    "    return tokens\n",
    "\n",
    "def label_tokens1(input_tokens, structure_text):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attribute_values = {\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\", \"QUANTITY\"}\n",
    "    structure_map = {}\n",
    "    for attribute in attribute_values:\n",
    "        # Match the attribute and its value in the structure text\n",
    "        pattern = r\"\\(\\s*\"+ attribute + r\"\\s+([^\\)]*)\\s*\\)\"\n",
    "        matches = re.finditer(pattern, structure_text)\n",
    "        for match in matches:\n",
    "            value = match.group(1).strip()\n",
    "            # Special handling for TOPPING with \"not\" before it\n",
    "            if attribute == \"TOPPING\":\n",
    "                preceding_text = structure_text[:match.start()]\n",
    "                if re.search(r\"\\bNOT\\b\", preceding_text, re.IGNORECASE):\n",
    "                    attribute = \"NOT_TOPPING\"\n",
    "            structure_map[value] = attribute\n",
    "    labeled_output = []\n",
    "    labeled_output_nums = []\n",
    "    entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17, \"NONE\": 18}\n",
    "    for token in input_tokens:\n",
    "        label = \"NONE\"\n",
    "        if token in structure_map:\n",
    "            label = structure_map[token]\n",
    "            label = \"B_\" + label\n",
    "        # else check if it is part of the key\n",
    "        else:\n",
    "            for key in structure_map.keys():\n",
    "                if token in key.split():\n",
    "                    label = structure_map[key]\n",
    "                    if token == key.split()[0]:\n",
    "                        label = \"B_\" + label\n",
    "                    else:\n",
    "                        label = \"I_\" + label\n",
    "                    break\n",
    "        labeled_output.append((token, label))\n",
    "        labeled_output_nums.append(entity_to_num[label])\n",
    "    return labeled_output, labeled_output_nums\n",
    "\n",
    "def label_tokens2(input_tokens, structure_tokens):\n",
    "    \"\"\"\n",
    "    Labels the input text based on a structured representation and a list of attributes.\n",
    "\n",
    "    Args:\n",
    "        input_tokens: The tokenized input text.\n",
    "        structure_text: The structured text containing attributes and their values.\n",
    "\n",
    "    Returns:\n",
    "        A list of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    attributes = [\"PIZZAORDER\", \"DRINKORDER\", \"COMPLEX_TOPPING\"]\n",
    "    execluded = {\"NUMBER\", \"SIZE\", \"TOPPING\", \"STYLE\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"VOLUME\", \"QUANTITY\"}\n",
    "    curr = \"NONE\"\n",
    "    # I will also keep tracking \"(\" and \")\" to know when to change the current attribute to NONE\n",
    "    parentheses =0\n",
    "    is_begin = True\n",
    "    labels_mapping = {}\n",
    "    for token in structure_tokens:\n",
    "        if token in attributes:\n",
    "            curr = token\n",
    "            is_begin = True\n",
    "        elif token == \"(\":\n",
    "            parentheses += 1\n",
    "        elif token == \")\":\n",
    "            parentheses -= 1\n",
    "            if parentheses == 1:\n",
    "                curr = \"NONE\"\n",
    "        elif token not in execluded:\n",
    "            if curr == \"NONE\":\n",
    "                labels_mapping[token] = curr\n",
    "            elif is_begin:\n",
    "                labels_mapping[token] = \"B_\" + curr\n",
    "                is_begin = False\n",
    "            else:\n",
    "                labels_mapping[token] = \"I_\" + curr\n",
    "    labeled_output = []\n",
    "    labeled_output_nums =[]\n",
    "    intent_to_num = {\"I_PIZZAORDER\": 0, \"I_DRINKORDER\": 1, \"I_COMPLEX_TOPPING\": 2, \"B_PIZZAORDER\": 3, \"B_DRINKORDER\": 4, \"B_COMPLEX_TOPPING\": 5, \"NONE\": 6}\n",
    "    for token in input_tokens:\n",
    "        label = \"NONE\"\n",
    "        if token in labels_mapping:\n",
    "            label = labels_mapping[token]\n",
    "        labeled_output.append((token, label))\n",
    "        labeled_output_nums.append(intent_to_num[label])\n",
    "    return labeled_output, labeled_output_nums\n",
    "\n",
    "def label_input(input_text, structure_text1, structure_text2):\n",
    "    \"\"\"\n",
    "    It is a similar function to the previous one, but it is used for adding another layer for the input\n",
    "    which is the preprocessing of the input text and then tokenizing it.\n",
    "\n",
    "    Args:\n",
    "        input_text: The raw input text.\n",
    "        structure_text1: The structured text containing attributes and their values. (train.TOP-DECOUPLED)\n",
    "        structure_text2: The structured text containing attributes and their values. (train.TOP)\n",
    "    \n",
    "    Returns:\n",
    "        2 lists of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    cleaned_text = clean_string(input_text)\n",
    "    input_tokens = tokenize_string(cleaned_text)\n",
    "    labeled_output1 = label_tokens1(input_tokens, structure_text1)\n",
    "    structure2_tokens = tokenize_string(structure_text2)\n",
    "    labeled_output2 = label_tokens2(input_tokens, structure2_tokens)\n",
    "    return labeled_output1, labeled_output2\n",
    "\n",
    "def label_complete_input (input_list, structure_text1_list, structure_text2_list):\n",
    "    \"\"\"\n",
    "    It is a similar function to the previous one, but it takes inputs as lists of tokens instead of strings.\n",
    "\n",
    "    Args:\n",
    "        input_text: The raw input text.\n",
    "        structure_text1: The structured text containing attributes and their values. (train.TOP-DECOUPLED)\n",
    "        structure_text2: The structured text containing attributes and their values. (train.TOP)\n",
    "    \n",
    "    Returns:\n",
    "        3 lists of tuples where each token in the input text is paired with its corresponding label.\n",
    "    \"\"\"\n",
    "    labeled_output1 = []\n",
    "    labeled_output2 = []\n",
    "    for text, struct1, struct2 in zip(input_list, structure_text1_list, structure_text2_list):\n",
    "        cleaned_text = clean_string(text)\n",
    "        input_tokens = tokenize_string(cleaned_text)\n",
    "        _, labels = label_tokens1(input_tokens, struct1)\n",
    "        labeled_output1.append(labels)\n",
    "        structure2_tokens = tokenize_string(struct2)\n",
    "        _, labels = label_tokens2(input_tokens, structure2_tokens)\n",
    "        labeled_output2.append(labels)\n",
    "    return labeled_output1, labeled_output2\n",
    "\n",
    "\n",
    "def format_train(data):\n",
    "    \"\"\"\n",
    "    Builds a training corpus from a JSON-like dataset.\n",
    "    Extracts the \"train.SRC\" field from each item in the dataset.\n",
    "\n",
    "    Args:\n",
    "        data: List of dictionaries, where each dictionary contains a \"train.SRC\" key.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings representing the training corpus.\n",
    "    \"\"\"\n",
    "    src, top, decoupled = [], [], []\n",
    "    for d in data:\n",
    "        src.append(d[\"train.SRC\"])\n",
    "        top.append(d[\"train.TOP\"])\n",
    "        decoupled.append(d[\"train.TOP-DECOUPLED\"])\n",
    "    return src, top, decoupled\n",
    "\n",
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and loads its content into a Python object.\n",
    "\n",
    "    Args:\n",
    "        path: Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Parsed JSON data as a Python object.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def list_of_lists(sentences):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into a list of tokenized sentences.\n",
    "    Each sentence is split into individual words.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of strings where each string is a sentence.\n",
    "\n",
    "    Returns:\n",
    "        List of lists where each inner list contains tokens of a sentence.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sentence))\n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(\"../data/fixed_PIZZA_train.json\")\n",
    "src_tokenized,top_tokenized,dec_tokenized= build_train_corpus_from_json(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# formating input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entites_output_as_number_labels,intents_output_as_number_labels=label_complete_input(src_tokenized,top_tokenized,dec_tokenized)\n",
    "corpus=src_tokenized\n",
    "input_as_tokenized_string=list_of_lists(src_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# embeddings part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText  # For Word2Vec model\n",
    "import gensim  # General Gensim utilities\n",
    "import nltk  # For tokenization and natural language processing\n",
    "import json  # For handling JSON files\n",
    "# from transformers import BertTokenizer, BertModel  # BERT tokenizer and model\n",
    "# import torch  # For PyTorch tensors and operations\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters for the Word2Vec model\n",
    "VECTOR_SIZE = 50  # Size of word vectors\n",
    "WINDOW_SIZE = 5  # Context window size\n",
    "THREADS = 4  # Number of threads to use for training\n",
    "CUTOFF_FREQ = 1  # Minimum frequency for a word to be included in vocabulary\n",
    "EPOCHS = 100  # Number of training epochs\n",
    "\n",
    "def list_of_lists(sentences):\n",
    "    \"\"\"\n",
    "    Converts a list of sentences into a list of tokenized sentences.\n",
    "    Each sentence is split into individual words.\n",
    "\n",
    "    Args:\n",
    "        sentences: List of strings where each string is a sentence.\n",
    "\n",
    "    Returns:\n",
    "        List of lists where each inner list contains tokens of a sentence.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokenized_sentences.append(nltk.word_tokenize(sentence))\n",
    "    return tokenized_sentences\n",
    "\n",
    "def train_gensim_w2v_model(corpus):\n",
    "    \"\"\"\n",
    "    Trains a Word2Vec model on the given corpus of sentences.\n",
    "\n",
    "    Args:\n",
    "        corpus: List of sentences (strings).\n",
    "\n",
    "    Returns:\n",
    "        A trained Gensim Word2Vec model.\n",
    "    \"\"\"\n",
    "    tokenized_sentences = list_of_lists(corpus)\n",
    "    model = Word2Vec(\n",
    "        sentences=tokenized_sentences,\n",
    "        vector_size=VECTOR_SIZE,\n",
    "        window=WINDOW_SIZE,\n",
    "        min_count=CUTOFF_FREQ,\n",
    "        workers=THREADS,\n",
    "    )\n",
    "    model.build_vocab(tokenized_sentences)\n",
    "    model.train(\n",
    "        corpus_iterable=tokenized_sentences,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=EPOCHS,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def embed_gensim(model, word):\n",
    "    \"\"\"\n",
    "    Retrieves the word embedding for a given word using a trained Gensim model.\n",
    "    Works for both w2v and fastext.\n",
    "    Args:\n",
    "        model: Trained Gensim Word2Vec model.\n",
    "        word: Word to retrieve the embedding for.\n",
    "\n",
    "    Returns:\n",
    "        Word embedding as a vector.\n",
    "    \"\"\"\n",
    "    return model.wv[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = train_gensim_w2v_model(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "NUM_CLASSES=19\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=20\n",
    "HIDDEN_SIZE=64\n",
    "\n",
    "# Step 1: Custom Dataset for Large Data\n",
    "class LargeDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data \n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    #I believe we can transform words into embeddings here\n",
    "    embeddings=[]\n",
    "    for seq in sequences:\n",
    "        x=[]\n",
    "        for token in seq:\n",
    "            x.append(emb_model.wv[token])\n",
    "        embeddings.append(x)\n",
    "    sequences=embeddings\n",
    "    labels = [torch.tensor(label, dtype=torch.long) for label in labels]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
    "    sequences = [torch.tensor(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    return padded_sequences, padded_labels, lengths\n",
    "\n",
    "\n",
    "labels = entites_output_as_number_labels\n",
    "\n",
    "dataset = LargeDataset(input_as_tokenized_string, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "\n",
    "class LargeWordRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LargeWordRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_x)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Step 4: Training Loop\n",
    "model = LargeWordRNN(input_size=VECTOR_SIZE, hidden_size=HIDDEN_SIZE, num_classes=NUM_CLASSES)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(EPOCHS): \n",
    "    for padded_sequences, padded_labels, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(padded_sequences, lengths)\n",
    "        loss = criterion(outputs.view(-1, NUM_CLASSES), padded_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
