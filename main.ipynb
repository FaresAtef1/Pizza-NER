{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import feature_extractor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES=23\n",
    "BATCH_SIZE=64\n",
    "EPOCHS=10\n",
    "HIDDEN_SIZE=64\n",
    "VECTOR_SIZE = 200  # Size of word vectors\n",
    "WINDOW_SIZE = 5  # Context window size\n",
    "THREADS = 4  # Number of threads to use for training\n",
    "CUTOFF_FREQ = 1  # Minimum frequency for a word to be included in vocabulary\n",
    "TRAINING_SIZE = 10000  \n",
    "TEST_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data=utils.read_data(\"../data/fixed_PIZZA_train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = complete_data[:TRAINING_SIZE]\n",
    "corpus, top, decoupled = utils.get_train_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entites_output_as_number_labels,intents_output_as_number_labels, input_as_tokenized_string=utils.label_complete_input(corpus, decoupled, top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_tokens = []\n",
    "with open('../data/food_review.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        reviews_tokens.append(utils.tokenize_string(line.strip()))\n",
    "print(\"Done1\")\n",
    "with open('../data/food_review2.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        reviews_tokens.append(utils.tokenize_string(line.strip()))\n",
    "print(\"Done2\")\n",
    "all_trainig = []\n",
    "all_corpus, _, _ = utils.get_train_dataset(complete_data)\n",
    "for i in range(len(all_corpus)):\n",
    "    all_trainig.append(utils.tokenize_string(all_corpus[i])) \n",
    "print(\"Done3\")\n",
    "emb_model = feature_extractor.train_gensim_w2v_model(all_trainig+reviews_tokens, VECTOR_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText  # For Word2Vec model\n",
    "emb_model.save('../embedding_models/word2vec_whole.model')\n",
    "# emb_model = Word2Vec.load('../models/word2vec_food_reviews.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_trainig+reviews_tokens), +10000+2456446)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model.wv.most_similar('coca-cola')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LargeDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data \n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    #I believe we can transform words into embeddings here\n",
    "    embeddings=[]\n",
    "    # print(sequences)\n",
    "    for seq in sequences:\n",
    "        x=[]\n",
    "        for token in seq:\n",
    "            x.append(emb_model.wv[token])\n",
    "        embeddings.append(x)\n",
    "    sequences=embeddings\n",
    "    labels = [torch.tensor(label, dtype=torch.long) for label in labels]\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
    "    sequences = [torch.tensor(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    return padded_sequences, padded_labels, lengths\n",
    "\n",
    "class LargeWordRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LargeWordRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_x)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if CUDA is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.is_available()) \n",
    "print(torch.version.cuda)\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "labels = entites_output_as_number_labels\n",
    "\n",
    "dataset = LargeDataset(input_as_tokenized_string, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "\n",
    "model = LargeWordRNN(input_size=VECTOR_SIZE, hidden_size=HIDDEN_SIZE, num_classes=NUM_CLASSES).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(EPOCHS): \n",
    "    for padded_sequences, padded_labels, lengths in dataloader:\n",
    "        padded_sequences=padded_sequences.to(device)\n",
    "        padded_labels=padded_labels.to(device)\n",
    "        lengths=lengths.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(padded_sequences, lengths)\n",
    "        loss = criterion(outputs.view(-1, NUM_CLASSES), padded_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data=complete_data[200000:200010]\n",
    "# print(test_data)\n",
    "# print(\"---------------------------------------------\")\n",
    "# test_corpus,_,_= utils.get_train_dataset(test_data)\n",
    "# print(test_corpus)\n",
    "# test_as_tokenized_string=feature_extractor.list_of_lists(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_as_tokenized_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TestLargeDataset(Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data \n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.data[idx]\n",
    "\n",
    "# def test_collate_fn(batch):\n",
    "#     sequences = batch\n",
    "#     #I believe we can transform words into embeddings here\n",
    "#     embeddings=[]\n",
    "#     for seq in sequences:\n",
    "#         print(seq)\n",
    "#         x=[]\n",
    "#         for token in seq:\n",
    "#             x.append(emb_model.wv[token])\n",
    "#         embeddings.append(x)\n",
    "#     sequences=embeddings\n",
    "#     sequences = [torch.tensor(seq) for seq in sequences]\n",
    "#     padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "#     lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "#     return padded_sequences, lengths\n",
    "\n",
    "\n",
    "# test_as_tokenized_string\n",
    "# dataset = TestLargeDataset(test_as_tokenized_string)\n",
    "# dataloader = DataLoader(dataset, batch_size=1, collate_fn=test_collate_fn, shuffle=False, num_workers=0)\n",
    "\n",
    "# for padded_sequences, lengths in dataloader:\n",
    "#     print(\"-------------------------------\")\n",
    "#     padded_sequences=padded_sequences.to(device)\n",
    "#     lengths=lengths.to(device)\n",
    "#     outputs = model(padded_sequences, lengths)\n",
    "#     entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17, \"NONE\": 18}\n",
    "#     for i, out in enumerate(outputs[0]):\n",
    "#         num = torch.argmax(out).int().item()\n",
    "#         for key, value in entity_to_num.items():\n",
    "#             if value == num:\n",
    "#                 print(key)\n",
    "#                 break\n",
    "\n",
    "\n",
    "    \n",
    "# #  entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17, \"NONE\": 18}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17, \"NONE\": 18}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data=utils.read_data(\"../data/fixed_PIZZA_dev.json\")\n",
    "dev_corpus, dev_top = utils.get_dev_dataset(dev_data)\n",
    "gold_dev_labels,dev_as_tokenized_string=utils.label_complete_dev(dev_corpus, dev_top)\n",
    "\n",
    "class TestLargeDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def test_collate_fn(batch):\n",
    "    sequences = batch\n",
    "    #I believe we can transform words into embeddings here\n",
    "    embeddings=[]\n",
    "    for seq in sequences:\n",
    "        print(seq)\n",
    "        x=[]\n",
    "        for token in seq:\n",
    "            x.append(emb_model.wv[token])\n",
    "        embeddings.append(x)\n",
    "    sequences=embeddings\n",
    "    sequences = [torch.tensor(seq) for seq in sequences]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    return padded_sequences, lengths\n",
    "\n",
    "dev_dataset = TestLargeDataset(dev_as_tokenized_string)\n",
    "dataloader = DataLoader(dev_dataset, batch_size=1, collate_fn=test_collate_fn, shuffle=False, num_workers=0)\n",
    "model_output=[]\n",
    "for padded_sequences, lengths in dataloader:\n",
    "    print(\"-------------------------------\")\n",
    "    labels = []\n",
    "    padded_sequences=padded_sequences.to(device)\n",
    "    lengths=lengths.to(device)\n",
    "    outputs = model(padded_sequences, lengths)\n",
    "    entity_to_num = {\"I_NUMBER\": 0, \"I_SIZE\": 1, \"I_TOPPING\": 2, \"I_STYLE\": 3, \"I_DRINKTYPE\": 4, \"I_CONTAINERTYPE\": 5, \"I_VOLUME\": 6, \"I_QUANTITY\": 7, \"B_NUMBER\": 8, \"B_SIZE\": 9, \"B_TOPPING\": 10, \"B_STYLE\": 11, \"B_DRINKTYPE\": 12, \"B_CONTAINERTYPE\": 13, \"B_VOLUME\": 14, \"B_QUANTITY\": 15, \"I_NOT_TOPPING\": 16, \"B_NOT_TOPPING\": 17,\"I_NOT_STYLE\": 18, \"B_NOT_STYLE\": 19, \"B_NOT_QUANTITY\": 20, \"I_NOT_QUANTITY\": 21, \"NONE\": 22}\n",
    "    for i, out in enumerate(outputs[0]):\n",
    "        num = torch.argmax(out).int().item()\n",
    "        labels.append(num)\n",
    "    model_output.append(labels)\n",
    "\n",
    "confusion_matrix, accuracy=utils.calc_accuracy(model_output, gold_dev_labels)\n",
    "print(confusion_matrix)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import os\n",
    "# import opendatasets as od\n",
    "# # ! pip install opendatasets\n",
    "# # Assign the Kaggle data set URL into variable\n",
    "# dataset = 'https://www.kaggle.com/datasets/ghaithqoudeiamti/pizza-dataset?select=food_review2.txt'\n",
    "# # Using opendatasets let's download the data sets\n",
    "# od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pickle.load(open('../models/model_1m.pk1' , 'rb'))\n",
    "# model = torch.load('../models/model_1m.pk1', map_location=torch.device('cpu'))\n",
    "model = torch.load('../models/model_1m.pk1', map_location=torch.device('cpu'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
