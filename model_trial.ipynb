{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "INPUT_SIZE=10\n",
    "HIDDEN_SIZE=5\n",
    "OUTPUT_SIZE=2\n",
    "BATCH_SIZE=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a two layer rnn\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size):\n",
    "        super(SimpleRNN,self).__init__()\n",
    "        self.rnn=nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.hidden_size=hidden_size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax=nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        h0 = torch.zeros(1, x.size(0), self.hidden_size)  # Initialize hidden state\n",
    "        out, _ = self.rnn(x, h0)  # Get RNN output for all time steps\n",
    "        out = self.fc(out)  # Apply fully connected layer to each time step\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn=SimpleRNN(INPUT_SIZE,HIDDEN_SIZE,OUTPUT_SIZE)\n",
    "h=rnn.init_hidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8117, 0.6684, 0.0676,  ..., 0.7028, 0.7567, 0.0788],\n",
      "         [0.4704, 0.7951, 0.9078,  ..., 0.8409, 0.5486, 0.4383],\n",
      "         [0.3530, 0.6708, 0.3745,  ..., 0.8771, 0.4568, 0.1929],\n",
      "         ...,\n",
      "         [0.5084, 0.5360, 0.5377,  ..., 0.3886, 0.7160, 0.4137],\n",
      "         [0.4758, 0.2053, 0.1933,  ..., 0.1102, 0.6248, 0.0098],\n",
      "         [0.0692, 0.5833, 0.7944,  ..., 0.3423, 0.0401, 0.1711]],\n",
      "\n",
      "        [[0.0776, 0.2932, 0.8699,  ..., 0.3744, 0.4937, 0.2988],\n",
      "         [0.1484, 0.2413, 0.7386,  ..., 0.6112, 0.3057, 0.1031],\n",
      "         [0.6146, 0.1556, 0.2657,  ..., 0.3628, 0.4610, 0.0488],\n",
      "         ...,\n",
      "         [0.4541, 0.0995, 0.8859,  ..., 0.8432, 0.5282, 0.4867],\n",
      "         [0.6444, 0.0085, 0.3449,  ..., 0.8685, 0.2367, 0.3958],\n",
      "         [0.7347, 0.4754, 0.2471,  ..., 0.6975, 0.4526, 0.4363]],\n",
      "\n",
      "        [[0.7008, 0.0871, 0.2066,  ..., 0.6063, 0.3991, 0.4072],\n",
      "         [0.8020, 0.9648, 0.9956,  ..., 0.4414, 0.0656, 0.1389],\n",
      "         [0.9090, 0.6608, 0.9912,  ..., 0.9479, 0.4992, 0.7575],\n",
      "         ...,\n",
      "         [0.2689, 0.9812, 0.8622,  ..., 0.0788, 0.2144, 0.6964],\n",
      "         [0.5211, 0.9982, 0.6562,  ..., 0.9205, 0.1678, 0.3302],\n",
      "         [0.6281, 0.4463, 0.5683,  ..., 0.3784, 0.4816, 0.9683]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.5529, 0.4411, 0.2658,  ..., 0.4093, 0.3893, 0.6909],\n",
      "         [0.4074, 0.3778, 0.8268,  ..., 0.0675, 0.1659, 0.1175],\n",
      "         [0.0480, 0.6759, 0.6323,  ..., 0.0720, 0.8118, 0.1058],\n",
      "         ...,\n",
      "         [0.1994, 0.5335, 0.1208,  ..., 0.3168, 0.8839, 0.1924],\n",
      "         [0.5650, 0.5346, 0.6648,  ..., 0.6602, 0.9599, 0.9079],\n",
      "         [0.1414, 0.1765, 0.1621,  ..., 0.5406, 0.5811, 0.5356]],\n",
      "\n",
      "        [[0.7047, 0.4303, 0.7111,  ..., 0.1598, 0.2134, 0.8621],\n",
      "         [0.1291, 0.7839, 0.0247,  ..., 0.9559, 0.5981, 0.7565],\n",
      "         [0.0935, 0.6518, 0.8112,  ..., 0.5825, 0.2034, 0.3962],\n",
      "         ...,\n",
      "         [0.5342, 0.5116, 0.2924,  ..., 0.8583, 0.7761, 0.9956],\n",
      "         [0.2609, 0.4650, 0.0578,  ..., 0.3068, 0.8398, 0.3020],\n",
      "         [0.4878, 0.0528, 0.8482,  ..., 0.4858, 0.2537, 0.1376]],\n",
      "\n",
      "        [[0.2708, 0.8381, 0.3159,  ..., 0.1597, 0.8829, 0.9050],\n",
      "         [0.1724, 0.7543, 0.0314,  ..., 0.1555, 0.8170, 0.4544],\n",
      "         [0.8188, 0.1240, 0.0905,  ..., 0.6664, 0.1636, 0.6534],\n",
      "         ...,\n",
      "         [0.3237, 0.9268, 0.9080,  ..., 0.1339, 0.9713, 0.4131],\n",
      "         [0.8478, 0.4466, 0.8103,  ..., 0.8459, 0.2549, 0.0040],\n",
      "         [0.1769, 0.0447, 0.5339,  ..., 0.3453, 0.4910, 0.6584]]])\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "T=3000\n",
    "x=torch.rand(BATCH_SIZE,T,INPUT_SIZE)\n",
    "output=torch.zeros((BATCH_SIZE,T,OUTPUT_SIZE))\n",
    "print(x)\n",
    "for i in range(500):\n",
    "    hidden = rnn.init_hidden()\n",
    "    rnn.zero_grad()\n",
    "    x=torch.rand(T,INPUT_SIZE,requires_grad=True)\n",
    "    optimizer = optim.Adam(rnn.parameters())\n",
    "    gold_tensor=torch.zeros(T, dtype=torch.long)\n",
    "    predictions\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "\n",
    "# print(loss)\n",
    "def train(gold_tensor, input_tensor):\n",
    "    rnn.zero_grad()\n",
    "    for i in range(T):\n",
    "        output, hidden = rnn(input_tensor[i], hidden)\n",
    "    loss = criterion(output, gold_tensor)\n",
    "    loss.backward(retain_graph=True)\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "    return output, loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.6205\n",
      "Epoch [20/100], Loss: 0.4905\n",
      "Epoch [30/100], Loss: 0.1018\n",
      "Epoch [40/100], Loss: 0.0109\n",
      "Epoch [50/100], Loss: 0.0028\n",
      "Epoch [60/100], Loss: 0.0014\n",
      "Epoch [70/100], Loss: 0.0009\n",
      "Epoch [80/100], Loss: 0.0008\n",
      "Epoch [90/100], Loss: 0.0007\n",
      "Epoch [100/100], Loss: 0.0006\n",
      "Predictions:\n",
      "tensor([[1, 0, 1],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Step 1: Prepare Variable-Length Sequences\n",
    "# Example sequences (word embeddings with 5 features per word)\n",
    "sequences = [\n",
    "    torch.tensor([[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                  [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "                  [0.2, 0.3, 0.4, 0.5, 0.6]], dtype=torch.float32),  # Length 3\n",
    "    torch.tensor([[0.3, 0.4, 0.5, 0.6, 0.7],\n",
    "                  [0.7, 0.6, 0.5, 0.4, 0.3]], dtype=torch.float32),  # Length 2\n",
    "    torch.tensor([[0.4, 0.5, 0.6, 0.7, 0.8]], dtype=torch.float32)  # Length 1\n",
    "]\n",
    "# Corresponding word-level labels\n",
    "labels = [\n",
    "    torch.tensor([1, 0, 1], dtype=torch.long),  # Length 3\n",
    "    torch.tensor([0, 1], dtype=torch.long),     # Length 2\n",
    "    torch.tensor([1], dtype=torch.long)         # Length 1\n",
    "]\n",
    "\n",
    "# Pad sequences and labels\n",
    "padded_sequences = pad_sequence(sequences, batch_first=True)  # (batch_size, max_seq_len, input_size)\n",
    "padded_labels = pad_sequence(labels, batch_first=True, padding_value=-1)  # Use -1 as ignore index\n",
    "\n",
    "# Compute sequence lengths\n",
    "sequence_lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "\n",
    "# Step 2: Define the Model\n",
    "class WordRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(WordRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # *2 for bidirectional\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_x)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 5  # Word embedding size\n",
    "hidden_size = 16\n",
    "num_classes = 2  # Binary classification\n",
    "learning_rate = 0.01\n",
    "num_epochs = 100\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = WordRNN(input_size, hidden_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Ignore padding in loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 3: Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    predictions = model(padded_sequences, sequence_lengths)\n",
    "    loss = criterion(predictions.view(-1, num_classes), padded_labels.view(-1))  # Flatten for loss calculation\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 4: Evaluate the Model\n",
    "with torch.no_grad():\n",
    "    predictions = model(padded_sequences, sequence_lengths)\n",
    "    predictions = predictions.argmax(dim=-1)  # Get predicted class per word\n",
    "    print(\"Predictions:\")\n",
    "    print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.7120\n",
      "Epoch 2, Loss: 0.7087\n",
      "Epoch 3, Loss: 0.7056\n",
      "Epoch 4, Loss: 0.7025\n",
      "Epoch 5, Loss: 0.6996\n",
      "Epoch 6, Loss: 0.6967\n",
      "Epoch 7, Loss: 0.6939\n",
      "Epoch 8, Loss: 0.6912\n",
      "Epoch 9, Loss: 0.6885\n",
      "Epoch 10, Loss: 0.6859\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Step 1: Custom Dataset for Large Data\n",
    "class LargeDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data \n",
    "        self.labels = labels \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sequences, labels = zip(*batch)\n",
    "    #I believe we can transform words into embeddings here\n",
    "    sequences = [torch.tensor(seq, dtype=torch.float32) for seq in sequences]\n",
    "    labels = [torch.tensor(label, dtype=torch.long) for label in labels]\n",
    "    padded_sequences = pad_sequence(sequences, batch_first=True)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-1)\n",
    "    lengths = torch.tensor([len(seq) for seq in sequences], dtype=torch.long)\n",
    "    return padded_sequences, padded_labels, lengths\n",
    "\n",
    "data = [\n",
    "    [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]],\n",
    "    [[0.2, 0.3], [0.4, 0.5]],\n",
    "    [[0.1, 0.2]]\n",
    "]\n",
    "labels = [\n",
    "    [1, 0, 1],\n",
    "    [0, 1],\n",
    "    [1]\n",
    "]\n",
    "\n",
    "dataset = LargeDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn, shuffle=True, num_workers=0)\n",
    "\n",
    "class LargeWordRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LargeWordRNN, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x, lengths):\n",
    "        packed_x = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed_x)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Step 4: Training Loop\n",
    "model = LargeWordRNN(input_size=2, hidden_size=64, num_classes=2)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(10): \n",
    "    for padded_sequences, padded_labels, lengths in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(padded_sequences, lengths)\n",
    "        loss = criterion(outputs.view(-1, 2), padded_labels.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
